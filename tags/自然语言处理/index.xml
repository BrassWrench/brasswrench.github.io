<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>自然语言处理 on 黄铜扳手的图书馆</title>
        <link>https://www.brasswrench.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</link>
        <description>Recent content in 自然语言处理 on 黄铜扳手的图书馆</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>黄铜扳手</copyright>
        <lastBuildDate>Wed, 13 Nov 2024 02:04:54 +0800</lastBuildDate><atom:link href="https://www.brasswrench.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>自然语言处理（二）：自然语言处理基础</title>
        <link>https://www.brasswrench.com/posts/artificial_intelligence/natural_language_processing/fundamentals_of_natural_language_processing/</link>
        <pubDate>Wed, 13 Nov 2024 01:11:04 +0800</pubDate>
        
        <guid>https://www.brasswrench.com/posts/artificial_intelligence/natural_language_processing/fundamentals_of_natural_language_processing/</guid>
        <description>&lt;img src="https://www.brasswrench.com/true" alt="Featured image of post 自然语言处理（二）：自然语言处理基础" /&gt;







&lt;iframe
  class=&#34;netease-cloud-music-frame&#34;
  title=&#34;song&amp;#39;s 2067668285&#34;
  frameborder=&#34;no&#34;
  border=&#34;0&#34;
  marginwidth=&#34;0&#34;
  marginheight=&#34;0&#34;
  width=&#34;100%&#34;
  height=&#34;86&#34;
  loading=&#34;lazy&#34;
  src=&#34;https://music.163.com/outchain/player?type=2&amp;amp;id=2067668285&amp;amp;auto=0&amp;amp;height=66&#34;&gt;
&lt;/iframe&gt;

&lt;h1 id=&#34;自然语言处理基础&#34;&gt;自然语言处理基础
&lt;/h1&gt;&lt;p&gt;  本章首先介绍自然语言处理中最基础、最本质的问题，即文本如何在计算机内表示，才能达到易于处理和计算的目的。其中，词的表示大体经过了早期的独热（One-hot）表示，到后来的分布式表示，再到最近的词向量三个阶段。至于更长文本的表示方法，本章只对最简单的词袋模型加以介绍，后续章节将介绍其他更好的表示方法。接着介绍三大类自然语言处理任务，即：语言模型、基础任务以及应用任务。其中，基础任务包括中文分词、词性标注、句法分析和语义分析等，应用任务包括信息抽取、情感分析、问答系统、机器翻译和对话系统等。由于这些任务基本可以归纳为文本分类、结构预测和序列到序列三大类问题，所以同时介绍这三大类问题的解决思路。最后，介绍自然语言处理任务的评价方法，主要包括针对确定答案的准确率和F值，针对非确定答案的BLEU值，以及针对开放答案的人工评价等。&lt;/p&gt;
&lt;h2 id=&#34;文本的表示&#34;&gt;文本的表示
&lt;/h2&gt;&lt;p&gt;  若要利用计算机对自然语言进行处理，首先需要解决语言（本书特指文本）
在计算机内部的存储和计算问题。&lt;strong&gt;字符串（String）&lt;/strong&gt;‌是文本最自然，也是最常用的机内存储形式。所谓字符串，即字符序列，而其中的一个字符本质上就是一个整数。基于字符串的文本表示方式可以实现简单的字符串增删改查等编辑任务，并能够通过编辑距离等算法计算两个字符串之间的字面相似度。在使用字符串表示（也叫符号表示）计算文本的语义信息时，往往需要使用基于规则的方法。例如,要判断一个句子的情感极性（褒义或贬义），规则的形式可能为：如果句子中出现“喜欢””漂亮”等词则为褒义；如果出现“讨厌”“丑陋&amp;quot;等词则为贬义。&lt;br&gt;
  这种基于规则的方法存在很多问题。首先，规则的归纳依赖专家的经验，需 要花费大量的人力、物力和财力；其次，规则的表达能力有限，很多语言现象无法用简单的规则描述；最后，随着规则的增多，规则之间可能存在矛盾和冲突的情况，导致最终无法做出决策。例如，一个句子中既出现了“喜欢”，又出现了“讨厌”，那么其极性应该是什么呢？&lt;br&gt;
  为了解决基于规则的方法存在的以上诸多问题，基于机器学习的自然语言处理技术应运而生，其最本质的思想是将文本表示为向量，其中的每一维代表一个特征。在进行决策的时候，只要对这些特征的相应值进行加权求和，就可以得到一个分数用于最终的判断。仍然以情感极性识别为例，一种非常简单的将原始文本表示为向量的方法为：令向量 \(\bm{x}\) 的每一维表示某个词在该文本中出现的次数,如 \(x_1\) 表示“我”出现的次数， \(x_2\) 表示“喜欢”出现的次数， \(x_3\) 表示“电影”出现的次数， \(x_4\) 表示“讨厌”出现的次数等，如果某个词在该句中没有出现，则相应的维数被设置为0。可见，输入向量 \(\bm{x}\) 的大小恰好为整个词表（所有不相同的词）的大小。然后就可以根据每个词对判断情感极性的重要性进行加权，如“喜欢”（ \(x_2\) ）对应的权重 \(\omega_2\) 可能比较大，而“讨厌&amp;quot;（ \(x_4\) ）对应的权重 \(\omega_4\) 可能比较小（可以为负数），对于情感极性影响比较小的词，如“我”“电影”等，对应的权重可能会趋近于 \(0\) 。这种文本表示的方法是两种技术的组合，即词的独热表示和文本的词袋表示。除了可以应用于基于机器学习的方法，文本向量表示还可以用于计算两个文本之间的相似度，即使用余弦函数等度量函数表示两个向量之间的相似度，并应用于信息检索等任务。下面就以上提到的各项技术分别进行详细的介绍。&lt;/p&gt;
&lt;h3 id=&#34;词的独热表示&#34;&gt;词的独热表示
&lt;/h3&gt;&lt;p&gt;  所谓词的独热表示，即使用一个词表大小的向量表示一个词（假设词表为 \(
\mathbb{V}\) ，则其大小为 \(|\mathbb{V}|\) ),然后将词表中的第 \(i\) 个词 \(\omega_i\) 表示为向量：&lt;/p&gt;
&lt;span id=&#34;20241113014020CST&#34;&gt;&lt;/span&gt;\[
\bm{e}_{\omega_i}=[0,0,\cdots,\underbrace{1}_{第i个词},\cdots,0] \in \set{0,1}^{|\mathbb{V}|}
\tag{1}
    \]
&lt;p&gt;  在该向量中，词表中第 \(i\) 个词在第 \(i\) 维上被设置为 \(1\) ，其余维均为 \(0\) 。这种表示被称为词的&lt;strong&gt;独热表示&lt;/strong&gt;或&lt;strong&gt;独热编码（One-hot Encoding）&lt;/strong&gt;。&lt;br&gt;
  独热表示的一个主要问题就是不同词使用完全不同的向量进行表示，这会导 致即使两个词在语义上很相似，但是通过余弦函数来度量它们之间的相似度时值却为 \(0\) 。另外，当应用于基于机器学习的方法时，独热模型会导致&lt;strong&gt;数据稀疏（Data Sparsity）&lt;/strong&gt;‌问题。例如，假设在训练数据中只见过“漂亮”，在测试数据中出现了
“美丽”，虽然它们之间很相似，但是系统仍然无法恰当地对“美丽”进行加权。由于数据稀疏问题，导致当训练数据规模有限时，很多语言现象没有被充分地学习到。&lt;br&gt;
  为了缓解数据稀疏问题，传统的做法是除了词自身，再提取更多和词相关的泛化特征，如词性特征、词义特征和词聚类特征等。以语义特征为例，通过引入WordNet等语义词典，可以获知“漂亮”和“美丽”是同义词，然后引入它们的共同语义信息作为新的额外特征，从而缓解同义词的独热表示不同的问题。可以说，在使用传统机器学习方法解决自然语言处理问题时，研究者的很大一部分精力都用在了挖掘有效的特征上。&lt;/p&gt;
&lt;h3 id=&#34;词的分布式表示&#34;&gt;词的分布式表示
&lt;/h3&gt;&lt;p&gt;  词的独热表示容易导致数据稀疏问题，而通过引入特征的方法虽然可以缓解
该问题，但是特征的设计费时费力。那么有没有办法自动提取特征并设置相应的特征值呢？&lt;/p&gt;
&lt;h4 id=&#34;分布式语义假设&#34;&gt;分布式语义假设
&lt;/h4&gt;&lt;p&gt;  人们在阅读过程中遇到从未见过的词时，通常会根据上下文来推断其含义以 及相关属性。基于这种思想，John Rupert Firth于1957年提出了&lt;strong&gt;分布式语义假设&lt;/strong&gt;：词的含义可由其上下文的分布进行表示。基于该思想，可以利用大规模的未标注文本数据，根据每个词的上下文分布对词进行表示。当然，分布式语义假设仅仅提供了一种语义建模的思想。具体到表示形式和上下文的选择，以及如何利用上下文的分布特征，都是需要解决的问题。&lt;br&gt;
  下面用一个具体的例子演示如何构建词的分布式表示。假设语料库中有以下三句话：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;我 喜欢 自然 语言 处理 。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;我 爱 深度 学习 。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;我 喜欢 机器 学习 。
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;  假设以词所在句子中的其他词语作为上下文，那么可以创建如&lt;a class=&#34;link&#34; href=&#34;#20241113015119CST&#34;&gt;表1&lt;/a&gt;所示的词语共现频次表。其中，词表 \(\mathbb{V}\) 包含“我”“喜欢”···“。”共10个词，即 \(|\mathbb{V}|=10\) 。表中的每一项代表一个词 \(w_i\) 与另一个词 \(w_j\) （上下文）在同一个句子中的共现频次，每个词与自身的共现频次设置为 \(0\) 。&lt;/p&gt;
&lt;span id=&#34;20241113015119CST&#34;&gt;&lt;/span&gt;&lt;div class=&#34;normal-table&#34;&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;我&lt;/th&gt;
          &lt;th&gt;喜欢&lt;/th&gt;
          &lt;th&gt;自然&lt;/th&gt;
          &lt;th&gt;语言&lt;/th&gt;
          &lt;th&gt;处理&lt;/th&gt;
          &lt;th&gt;爱&lt;/th&gt;
          &lt;th&gt;深度&lt;/th&gt;
          &lt;th&gt;学习&lt;/th&gt;
          &lt;th&gt;机器&lt;/th&gt;
          &lt;th&gt;。&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;我&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;3&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;喜欢&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;自然&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;语言&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;处理&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;爱&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;深度&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;学习&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;机器&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;。&lt;/td&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;&lt;p class=&#34;table-title&#34; style=&#34;text-align:center; margin-top: 0; margin-bottom:0&#34;&gt;表1&amp;emsp;词语共现频次表&lt;/p&gt;
&lt;p&gt;  表中的每一行代表一个词的向量。通过计算两个向量之间的余弦函数，就可 以计算两个词的相似度。如“喜欢”和“爱”，由于有共同的上下文“我”和“学习”，使得它们之间具有了一定的相似性，而不是如独热表示一样，没有任何关系。&lt;br&gt;
  除了词，上下文的选择有很多种方式，而选择不同的上下文得到的词向量表示性质会有所不同。例如，可以使用词在句子中的一个固定窗口内的词作为其上下文，也可以使用所在的文档本身作为上下文。前者得到的词表示将更多地反映词的局部性质：具有相似词法、句法属性的词将会具有相似的向量表示。而后者 将更多地反映词代表的主题信息。
不过，直接使用与上下文的共现频次作为词的向量表示，至少存在以下三个问题：&lt;br&gt;
◾高频词误导计算结果。如上例中，“我”“。”与其他词的共现频次很高，导致实际上可能没有关系的两个词由于都和这些词共现过，从而产生了较高的 相似度。&lt;br&gt;
◾共现频次无法反映词之间的高阶关系。例如，假设词“A”与”B”共现过，“B”与&amp;quot;C”共现过，“C”与“D&amp;quot;共现过，通过共现频次，只能获知“A”与“C”都与“B”共现过，它们之间存在一定的关系，而“A”与“D”这种高阶的关系则无法知晓。&lt;br&gt;
◾然存在稀疏性的问题。即向量中仍有大量的值为0，这一点从&lt;a class=&#34;link&#34; href=&#34;#20241113015119CST&#34;&gt;表1&lt;/a&gt;中也可以看出。&lt;/p&gt;
&lt;p&gt;  下面分别介绍如何通过点互信息和奇异值分解两种技术来解决这些问题。&lt;/p&gt;
&lt;h4 id=&#34;点互信息&#34;&gt;点互信息
&lt;/h4&gt;</description>
        </item>
        <item>
        <title>自然语言处理（一）：自然语言处理概论</title>
        <link>https://www.brasswrench.com/posts/artificial_intelligence/natural_language_processing/introduction_to_natural_language_processing/</link>
        <pubDate>Tue, 15 Oct 2024 19:05:40 +0800</pubDate>
        
        <guid>https://www.brasswrench.com/posts/artificial_intelligence/natural_language_processing/introduction_to_natural_language_processing/</guid>
        <description>&lt;img src="https://www.brasswrench.com/true" alt="Featured image of post 自然语言处理（一）：自然语言处理概论" /&gt;







&lt;iframe
  class=&#34;netease-cloud-music-frame&#34;
  title=&#34;song&amp;#39;s 1372188635&#34;
  frameborder=&#34;no&#34;
  border=&#34;0&#34;
  marginwidth=&#34;0&#34;
  marginheight=&#34;0&#34;
  width=&#34;100%&#34;
  height=&#34;86&#34;
  loading=&#34;lazy&#34;
  src=&#34;https://music.163.com/outchain/player?type=2&amp;amp;id=1372188635&amp;amp;auto=0&amp;amp;height=66&#34;&gt;
&lt;/iframe&gt;

&lt;h1 id=&#34;自然语言处理概论&#34;&gt;自然语言处理概论
&lt;/h1&gt;&lt;p&gt;  本篇首先介绍了自然语言以及自然语言处理的基本概念，并总结了自然语言处理所面临的8个难点，即语言的抽象性、组合性、歧义性、进化性、非规范性、主观性、知识性及难移植性。正是由于这些难点的存在，导致自然语言处理任务纷繁复杂，并产生了多种划分方式，如按照任务层级，可以分为资源建设、基础任务、应用任务及应用系统四个层级；按照任务类型，可以分为回归、分类、匹配、解析及生成五大问题；按照研究对象的不同，可以分为形式、语义、推理及语用分析四个等级。从历史上看，自然语言处理经过了将近60年的发展，期间经历了理性主义和经验主义两大发展阶段。其中，经验主义又被分成了基于统计模型、深度学习模型及最新的预训练模型三个阶段，尤其是&amp;quot;预训练+精调&amp;quot;的方式，已成为自然语言处理的最新范式。&lt;/p&gt;
&lt;h2 id=&#34;自然语言处理的概念&#34;&gt;自然语言处理的概念
&lt;/h2&gt;&lt;p&gt;  &lt;strong&gt;自然语言&lt;/strong&gt;通常指的是人类语言（本书特指文本符号，而非语音信号），是人类思维的载体和交流的基本工具，也是人类区别于动物的根本标志，更是人类智能发展的外在体现形式之一。&lt;strong&gt;自然语言处理（Natural Language Processing，简称NLP）&lt;/strong&gt;‌主要研究用计算机理解和生成自然语言的各种理论和方法，属于人工智能领域的一个重要甚至核心分支，是计算机科学与语言学的交叉学科，又常被称为&lt;strong&gt;计算语言学(Computational Linguistics，简称CL）&lt;/strong&gt;。随着互联网的快速发展，网络文本呈爆炸性增长，为自然语言处理提出了巨大的应用需求。同时，自然语言处理研究也为人们更深刻地理解语言的机理和社会的机制提供了一条重要的途径，因此具有重要的科学意义。&lt;br&gt;
  目前，人们普遍认为人工智能的发展经历了从运算智能到感知智能，再到认知智能三个发展阶段。运算智能关注的是机器的基础运算和存储能力，在这方面，机器已经完胜人类。感知智能则强调机器的模式识别能力，如语音的识别以及图像的识别，目前机器在感知智能上的水平基本达到甚至超过了人类的水平。然而,在涉及自然语言处理以及常识建模和推理等研究的认知智能上，机器与人类还有很大的差距。&lt;/p&gt;
&lt;h2 id=&#34;自然语言处理的难点&#34;&gt;自然语言处理的难点
&lt;/h2&gt;&lt;p&gt;  为什么计算机在处理自然语言时会如此困难呢？这主要是因为自然语言具有高度的抽象性、近乎无穷变化的语义组合性、无处不在的歧义性和进化性，以及理解语言通常需要背景知识和推理能力等，下面分别进行具体的介绍。&lt;/p&gt;
&lt;h3 id=&#34;抽象性&#34;&gt;抽象性
&lt;/h3&gt;&lt;p&gt;  语言是由抽象符号构成的，每个符号背后都对应着现实世界或人们头脑中的复杂概念，如&amp;quot;车”表示各种交通工具&amp;mdash;&amp;mdash;汽车、火车、自行车等，它们都具有共同的属性，有轮子、能载人或物等。&lt;/p&gt;
&lt;h3 id=&#34;组合性&#34;&gt;组合性
&lt;/h3&gt;&lt;p&gt;  每种语言的基本符号单元都是有限的，如英文仅有26个字母，中国国家标准GB2312《信息交换用汉字编码字符集•基本集》共收录6763个汉字，即便是常用的单词，英文和中文也不过各几十万个。然而，这些有限的符号却可以组合成无限的语义，即使是相同的词汇，由于顺序不同，组合的语义也是不相同的，因此无法使用穷举的方法实现对自然语言的理解。&lt;/p&gt;
&lt;h3 id=&#34;歧义性&#34;&gt;歧义性
&lt;/h3&gt;&lt;p&gt;  歧义性主要是由于语言的形式和语义之间存在多对多的对应关系导致的，如:“苹果”一词，既可以指水果，也可以指一家公司或手机、电脑等电子设备，这就是典型的一词多义现象。另外，对于两个句子，如“曹雪芹写了红楼梦”和&amp;quot;红楼梦的作者是曹雪芹&amp;quot;，虽然它们的形式不同，但是语义是相同的。&lt;/p&gt;
&lt;h3 id=&#34;进化性&#34;&gt;进化性
&lt;/h3&gt;&lt;p&gt;  任何一种“活着&amp;quot;的语言都是在不断发展变化的，即语言具有明显的进化性，也称创造性。这主要体现在两方面：一方面是新词汇层出不穷，如&amp;quot;超女”“非典”&amp;ldquo;新冠&amp;quot;等；另一方面则体现在旧词汇被赋予新的含义，如&amp;quot;腐败&amp;rdquo;“杯具”等。除了词汇，语言的语法等也在不断变化，新的用法层出不穷。&lt;/p&gt;
&lt;h3 id=&#34;非规范性&#34;&gt;非规范性
&lt;/h3&gt;&lt;p&gt;  在互联网上，尤其是在用户产生的内容中，经常有一些有意或无意造成的非规范文本，为自然语言处理带来了不小的挑战，如音近词（&amp;ldquo;为什么” \(\to\) “为森么&amp;rdquo;，&amp;ldquo;怎么了” \(\to\) &amp;ldquo;肿么了&amp;rdquo;）、单词的简写或变形（please \(\to\) pls、cool \(\to\) coooooooool）、新造词（“喜大普奔”“不明觉厉”）和错别字等。&lt;/p&gt;
&lt;h3 id=&#34;主观性&#34;&gt;主观性
&lt;/h3&gt;&lt;p&gt;  和感知智能问题不同，属于认知智能的自然语言处理问题往往具有一定的主观性，这不但提高了数据标注的难度，还为准确评价系统的表现带来了一定的困难。如在分词这一最基本的中文自然语言处理任务中，关于什么是&amp;quot;词”的定义都尚不明确，比如&amp;quot;打篮球&amp;quot;是一个词还是两个词呢？所以，在标注自然语言处理任务的数据时，往往需要对标注人员进行一定的培训，使得很难通过众包的方式招募大量的标注人员，导致自然语言处理任务的标注数据规模往往比图像识别、语音识别的标注数据规模要小得多。此外，由于不同的分词系统往往标准都不尽相同，所以通过准确率等客观指标对比不同的分词系统本身就是不客观的。难以评价的问题在人机对话等任务中体现得更为明显，由于对话回复的主观性，很难有一个所谓的标准回复，所以如何自动评价人机对话系统仍然是一个开放的问题。&lt;/p&gt;
&lt;h3 id=&#34;知识性&#34;&gt;知识性
&lt;/h3&gt;&lt;p&gt;  理解语言通常需要背景知识以及基于这些知识的推理能力。例如，针对句子&amp;quot;张三打了李四，然后他倒了&amp;rdquo;，问其中的&amp;quot;他&amp;quot;指代的是&amp;quot;张三&amp;quot;还是&amp;quot;李四”？只有具备了“被打的人更容易倒”这一知识，才能推出&amp;quot;他”很可能指代的是“李四”。而如果将&amp;quot;倒”替换为&amp;quot;笑”，则&amp;quot;他”很可能指代的是“张三”，因为“被打的人不太容易笑&amp;quot;。但是，如何表示、获取并利用这些知识呢？目前的自然语言处理技术并没有提供很好的答案。&lt;/p&gt;
&lt;h3 id=&#34;难移植性&#34;&gt;难移植性
&lt;/h3&gt;&lt;p&gt;  由于自然语言处理涉及的任务和领域众多，并且它们之间的差异较大，造成了难移植性的问题。如下一节将要介绍的，自然语言处理任务根据层级可以分为分词、词性标注、句法分析和语义分析等基础任务，以及信息抽取、问答系统和对话系统等应用任务，由于这些任务的目标和数据各不相同，很难使用统一的技术或模型加以解决，因此不得不针对不同的任务设计不同的算法或训练不同的模型。另外，由于不同领域的用词以及表达方式不尽相同，因此在一个领域上学习的模型也很难应用于其他领域，这也给提高自然语言处理系统的可移植性带来了极大的困难。&lt;/p&gt;
&lt;p&gt;  综上所述，由于自然语言处理面临的众多问题，使其成为目前制约人工智能取得更大突破和更广泛应用的瓶颈之一。因此自然语言处理又被誉为&amp;quot;人工智能皇冠上的明珠”，并吸引了越来越多的人工智能研究者加入。&lt;/p&gt;
&lt;h2 id=&#34;自然语言处理任务体系&#34;&gt;自然语言处理任务体系
&lt;/h2&gt;&lt;h3 id=&#34;任务层级&#34;&gt;任务层级
&lt;/h3&gt;&lt;p&gt;  如前所述，自然语言处理的一大特点是涉及的任务众多。按照从低层到高层的方式，可以划分为资源建设、基础任务、应用任务和应用系统四大类。其中，&lt;strong&gt;资源建设&lt;/strong&gt;主要包括两大类任务，即语言学知识库建设和语料库资源建设。所谓&lt;strong&gt;语言学知识库&lt;/strong&gt;，一般包括词典、规则库等。&lt;strong&gt;词典（Dictionary）&lt;/strong&gt;‌也称&lt;strong&gt;辞典（Thesaurus）&lt;/strong&gt;，除了可以为词语提供音韵、句法或者语义解释以及示例等信息，还可以提供词语之间的关系信息，如上下位、同义反义关系等。&lt;strong&gt;语料库资源&lt;/strong&gt;指的是面向某一自然语言处理任务所标注的数据。无论是语言学资源，还是语料库资源的建设，都是上层各种自然语言处理技术的基础，需要花费大量的人力和物力构建。&lt;strong&gt;基础任务&lt;/strong&gt;包括分词、词性标注、句法分析和语义分析等，这些任务往往不直接面向终端用户，除了语言学上的研究价值，它们主要为上层应用任务提供所需的特征。&lt;strong&gt;应用任务&lt;/strong&gt;包括信息抽取、情感分析、问答系统、机器翻译和对话系统等，它们往往可以作为产品直接被终端用户使用。本部分第2篇将对这些任务进行更详细的介绍。&lt;br&gt;
  &lt;strong&gt;应用系统&lt;/strong&gt;特指自然语言处理技术在某一领域的综合应用，又被称为NLP+，即自然语言处理技术加上特定的应用领域。如在智能教育领域，可以使用文本分类、回归等技术，实现主观试题的智能评阅，帮助教师减轻工作量，提高工作效率；在智慧医疗领域，自然语言处理技术可以帮助医生跟踪最新的医疗文献，帮助患者进行简单的自我诊断等；在智能司法领域，可以使用阅读理解、文本匹配等技术，实现自动量刑、类案检索和法条推荐等。总之，凡是涉及文本理解和生成的领域，自然语言处理技术都可以发挥巨大的作用。&lt;/p&gt;
&lt;span id=&#34;20241015201555CST&#34;&gt;&lt;/span&gt;&lt;div class=&#34;svg-figure&#34;&gt;
            &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; version=&#34;1.1&#34; viewBox=&#34;0 0 863.7 437.1&#34;&gt;
  &lt;defs&gt;
    &lt;style&gt;
      .cls-1 {
        opacity: .2;
      }

      .cls-1, .cls-2, .cls-3, .cls-4, .cls-5, .cls-6, .cls-7, .cls-8 {
        isolation: isolate;
      }

      .cls-1, .cls-4, .cls-7 {
        stroke: #000;
        stroke-miterlimit: 10;
      }

      .cls-2 {
        letter-spacing: 0em;
      }

      .cls-2, .cls-3, .cls-5, .cls-6, .cls-8 {
        font-size: 18px;
      }

      .cls-2, .cls-3, .cls-8 {
        font-family: NotoSerifSC-Regular, &#39;Noto Serif SC&#39;;
      }

      .cls-4 {
        opacity: .5;
      }

      .cls-4, .cls-7 {
        fill: none;
      }

      .cls-5 {
        letter-spacing: -.5em;
      }

      .cls-5, .cls-6 {
        font-family: NotoSerifSC-Bold, &#39;Noto Serif SC&#39;;
        font-weight: 700;
      }

      .cls-8 {
        letter-spacing: -.5em;
      }
    &lt;/style&gt;
  &lt;/defs&gt;
  &lt;!-- Generator: Adobe Illustrator 28.7.1, SVG Export Plug-In . SVG Version: 1.2.0 Build 142)  --&gt;
  &lt;g&gt;
    &lt;g id=&#34;_图层_1&#34;&gt;
      &lt;polygon class=&#34;cls-1&#34; points=&#34;383.6 6.6 213.8 428.7 553.5 428.7 383.6 6.6&#34;/&gt;
      &lt;rect class=&#34;cls-7&#34; x=&#34;356.2&#34; y=&#34;42.4&#34; width=&#34;293.7&#34; height=&#34;68.6&#34; rx=&#34;12&#34; ry=&#34;12&#34;/&gt;
      &lt;text class=&#34;cls-6&#34; transform=&#34;translate(369.1 70.9)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;应用系&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-5&#34; transform=&#34;translate(423.1 70.9)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;统&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(369.9 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;教&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-2&#34; transform=&#34;translate(387.9 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;育&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-8&#34; transform=&#34;translate(405.6 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;、&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(414.9 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;医&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-2&#34; transform=&#34;translate(432.9 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;疗&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-8&#34; transform=&#34;translate(450.6 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;、&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(459.9 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;司&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-2&#34; transform=&#34;translate(477.9 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;法&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-8&#34; transform=&#34;translate(495.6 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;、&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(504.9 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;金&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-2&#34; transform=&#34;translate(522.9 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;融&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-8&#34; transform=&#34;translate(540.6 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;、&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(549.9 99.5)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;机器人等&lt;/tspan&gt;&lt;/text&gt;
      &lt;rect class=&#34;cls-7&#34; x=&#34;356.2&#34; y=&#34;122.5&#34; width=&#34;293.7&#34; height=&#34;92.8&#34; rx=&#34;12&#34; ry=&#34;12&#34;/&gt;
      &lt;text class=&#34;cls-6&#34; transform=&#34;translate(369.1 151)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;应用任务&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(369.9 179.6)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;信息抽&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-2&#34; transform=&#34;translate(423.9 179.6)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;取&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-8&#34; transform=&#34;translate(441.6 179.6)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;、&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(450.9 179.6)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;情感分&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-2&#34; transform=&#34;translate(504.9 179.6)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;析&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-8&#34; transform=&#34;translate(522.6 179.6)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;、&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(531.9 179.6)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;问答系&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-2&#34; transform=&#34;translate(585.9 179.6)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;统&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-8&#34; transform=&#34;translate(603.6 179.6)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;、&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(369.9 201.2)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;机器翻译和对话系统等&lt;/tspan&gt;&lt;/text&gt;
      &lt;rect class=&#34;cls-7&#34; x=&#34;356.2&#34; y=&#34;231.7&#34; width=&#34;293.7&#34; height=&#34;87.8&#34; rx=&#34;12&#34; ry=&#34;12&#34;/&gt;
      &lt;text class=&#34;cls-6&#34; transform=&#34;translate(369.1 260.2)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;基础任务&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(369.9 288.8)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;分&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-2&#34; transform=&#34;translate(387.9 288.8)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;词&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-8&#34; transform=&#34;translate(405.6 288.8)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;、&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(414.9 288.8)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;词性标&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-2&#34; transform=&#34;translate(468.9 288.8)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;注&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-8&#34; transform=&#34;translate(486.6 288.8)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;、&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(495.9 288.8)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;句法分析和语义&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(369.9 310.4)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;分析等&lt;/tspan&gt;&lt;/text&gt;
      &lt;rect class=&#34;cls-4&#34; x=&#34;356.2&#34; y=&#34;335.8&#34; width=&#34;293.7&#34; height=&#34;68.6&#34; rx=&#34;12&#34; ry=&#34;12&#34;/&gt;
      &lt;text class=&#34;cls-6&#34; transform=&#34;translate(369.1 364.3)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;资源建设&lt;/tspan&gt;&lt;/text&gt;
      &lt;text class=&#34;cls-3&#34; transform=&#34;translate(369.9 392.9)&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;语言学知识库建设和语料资源建设&lt;/tspan&gt;&lt;/text&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/div&gt;&lt;p class=&#34;svg-title&#34; style=&#34;text-align:center; margin-top: 0; margin-bottom:0&#34;&gt;图1&amp;emsp;自然语言处理任务层级&lt;/p&gt;
&lt;h3 id=&#34;任务类别&#34;&gt;任务类别
&lt;/h3&gt;&lt;p&gt;  虽然自然语言处理任务多种多样，刚涉足该领域的人可能会觉得眼花缭乱、
无从下手，但是这些复杂的任务基本上都可以归纳为回归、分类、匹配、解析或 生成五类问题中的一种。下面分别加以介绍：&lt;/p&gt;
&lt;h4 id=&#34;回归问题&#34;&gt;回归问题
&lt;/h4&gt;&lt;p&gt;  即将输入文本映射为一个连续的数值，如对作文的打分，对案件刑期或罚款 金额的预测等。&lt;/p&gt;
&lt;h4 id=&#34;分类问题&#34;&gt;分类问题
&lt;/h4&gt;&lt;p&gt;  又称为文本分类，即判断一个输入的文本所属的类别，如：在垃圾邮件识别任务中，可以将一封邮件分为正常和垃圾两类；在情感分析中，可以将用户的情感分为褒义、贬义或中性三类。&lt;/p&gt;
&lt;h4 id=&#34;匹配问题&#34;&gt;匹配问题
&lt;/h4&gt;&lt;p&gt;  判断两个输入文本之间的关系，如：它们之间是复述或非复述两类关系；或
者蕴含、矛盾和无关三类关系。另外，识别两个输入文本之间的相似性（0到1的数值）也属于匹配问题。&lt;/p&gt;
&lt;h4 id=&#34;解析问题&#34;&gt;解析问题
&lt;/h4&gt;&lt;p&gt;  特指对文本中的词语进行标注或识别词语之间的关系，典型的解析问题包括 词性标注、句法分析等，另外还有很多问题，如分词、命名实体识别等也可以转化为解析问题。&lt;/p&gt;
&lt;h4 id=&#34;生成问题&#34;&gt;生成问题
&lt;/h4&gt;&lt;p&gt;  特指根据输入（可以是文本，也可以是图片、表格等其他类型数据）生成一段自然语言，如机器翻译、文本摘要、图像描述生成等都是典型的文本生成类任务。&lt;/p&gt;
&lt;h3 id=&#34;研究对象与层次&#34;&gt;研究对象与层次
&lt;/h3&gt;&lt;p&gt;  此外此外，也可以通过对研究对象的区分，将自然语言处理研究分成多个层次的任务。自然语言处理主要涉及“名”“实”“知”“境”之间的关系，如&lt;a class=&#34;link&#34; href=&#34;#20241113002341CST&#34;&gt;图2&lt;/a&gt;所示。其中&amp;quot;名&amp;quot;指的是语言符号；“实”表示客观世界中存在的事实或人的主观世界中的概念；&amp;ldquo;知”是指知识，包括常识知识、世界知识和领域知识等；“境”则是指语言所处的环境。&lt;/p&gt;
&lt;span id=&#34;20241113002341CST&#34;&gt;&lt;/span&gt;&lt;div class=&#34;svg-figure&#34;&gt;
            &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;svg id=&#34;_图层_1&#34; data-name=&#34;图层_1&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 863.7 378.1544&#34;&gt;
  &lt;g&gt;
    &lt;g&gt;
      &lt;rect x=&#34;359.85&#34; y=&#34;3.6984&#34; width=&#34;144&#34; height=&#34;69.931&#34; style=&#34;opacity: .2; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;text transform=&#34;translate(422.8499 44.2979)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;知&lt;/tspan&gt;&lt;/text&gt;
    &lt;/g&gt;
    &lt;g&gt;
      &lt;rect x=&#34;359.85&#34; y=&#34;141.3679&#34; width=&#34;144&#34; height=&#34;69.931&#34; style=&#34;opacity: .2; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;text transform=&#34;translate(422.8499 181.9674)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;名&lt;/tspan&gt;&lt;/text&gt;
    &lt;/g&gt;
    &lt;g&gt;
      &lt;rect x=&#34;139.4661&#34; y=&#34;141.3679&#34; width=&#34;144&#34; height=&#34;69.931&#34; style=&#34;opacity: .2; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;text transform=&#34;translate(202.466 181.9674)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;实&lt;/tspan&gt;&lt;/text&gt;
    &lt;/g&gt;
    &lt;g&gt;
      &lt;rect x=&#34;580.2339&#34; y=&#34;141.3679&#34; width=&#34;144&#34; height=&#34;69.931&#34; style=&#34;opacity: .2; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;text transform=&#34;translate(643.2338 181.9674)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;境&lt;/tspan&gt;&lt;/text&gt;
    &lt;/g&gt;
    &lt;text transform=&#34;translate(449.0502 114.0575)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;推理&lt;/tspan&gt;&lt;/text&gt;
    &lt;g&gt;
      &lt;polyline points=&#34;431.85 74.1975 419.9064 85.9393 426.9524 85.9393 426.9524 107.2851&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;polyline points=&#34;431.85 140.5819 419.9064 128.8401 426.9524 128.8401 426.9524 107.4944 426.9524 106.7545&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;polyline points=&#34;431.277 74.1975 443.2206 85.9393 436.1746 85.9393 436.1746 107.2851&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;polyline points=&#34;431.277 140.5819 443.2206 128.8401 436.1746 128.8401 436.1746 107.4944 436.1746 106.7545&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
    &lt;/g&gt;
    &lt;g&gt;
      &lt;polyline points=&#34;359.1069 178.6654 347.3651 166.7218 347.3651 173.7678 326.0193 173.7678&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;polyline points=&#34;283.7225 178.6654 295.4643 166.7218 295.4643 173.7678 316.81 173.7678 317.5499 173.7678&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;polyline points=&#34;359.1069 178.0924 347.3651 190.036 347.3651 182.99 326.0193 182.99&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;polyline points=&#34;283.7225 178.0924 295.4643 190.036 295.4643 182.99 316.81 182.99 317.5499 182.99&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;line x1=&#34;315.9841&#34; y1=&#34;173.7678&#34; x2=&#34;330.1746&#34; y2=&#34;173.7678&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;line x1=&#34;314.8413&#34; y1=&#34;182.99&#34; x2=&#34;331.0317&#34; y2=&#34;182.99&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
    &lt;/g&gt;
    &lt;g&gt;
      &lt;polyline points=&#34;579.9775 176.6198 568.2357 164.6762 568.2357 171.7222 546.89 171.7222&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;polyline points=&#34;504.5931 176.6198 516.3349 164.6762 516.3349 171.7222 537.6807 171.7222 538.4205 171.7222&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;polyline points=&#34;579.9775 176.0468 568.2357 187.9905 568.2357 180.9444 546.89 180.9444&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;polyline points=&#34;504.5931 176.0468 516.3349 187.9905 516.3349 180.9444 537.6807 180.9444 538.4205 180.9444&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;line x1=&#34;536.8548&#34; y1=&#34;171.7222&#34; x2=&#34;551.0452&#34; y2=&#34;171.7222&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;line x1=&#34;535.7119&#34; y1=&#34;180.9444&#34; x2=&#34;551.9024&#34; y2=&#34;180.9444&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
    &lt;/g&gt;
    &lt;g&gt;
      &lt;path d=&#34;M402.4139,215.6793l16.7481-.1427-4.9823,4.9823,15.0937,15.0937s.134.134.271.2616l15.0937-15.0937-4.9823-4.9823,16.7481.1427&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
      &lt;path d=&#34;M402.8191,215.2741l-.1427,16.7481,4.9823-4.9823,15.0937,15.0937.5232.5232,6.2643,6.2643s6.5257-6.5259,6.5257-6.5259l15.0937-15.0937,4.9823,4.9823-.1427-16.7481&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
    &lt;/g&gt;
    &lt;text transform=&#34;translate(371.8901 114.0577)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;（3&lt;/tspan&gt;&lt;tspan x=&#34;26.7842&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.477em;&#34;&gt;）&lt;/tspan&gt;&lt;/text&gt;
    &lt;text transform=&#34;translate(299.0229 166.9735)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;（2&lt;/tspan&gt;&lt;tspan x=&#34;26.7842&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.477em;&#34;&gt;）&lt;/tspan&gt;&lt;/text&gt;
    &lt;text transform=&#34;translate(303.4145 205.7082)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;语义&lt;/tspan&gt;&lt;/text&gt;
    &lt;text transform=&#34;translate(519.8934 163.9678)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;（2&lt;/tspan&gt;&lt;tspan x=&#34;26.7842&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.477em;&#34;&gt;）&lt;/tspan&gt;&lt;/text&gt;
    &lt;text transform=&#34;translate(524.2849 202.7024)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;语用&lt;/tspan&gt;&lt;/text&gt;
    &lt;text transform=&#34;translate(391.4584 269.0736)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;（1&lt;/tspan&gt;&lt;tspan x=&#34;26.7842&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.477em;&#34;&gt;）&lt;/tspan&gt;&lt;tspan x=&#34;36.1978&#34; y=&#34;0&#34;&gt;形式&lt;/tspan&gt;&lt;/text&gt;
  &lt;/g&gt;
  &lt;g&gt;
    &lt;text transform=&#34;translate(135.7314 309.3948)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;名&lt;/tspan&gt;&lt;tspan x=&#34;18&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.5em;&#34;&gt;：&lt;/tspan&gt;&lt;tspan x=&#34;27&#34; y=&#34;0&#34;&gt;语言符号&lt;/tspan&gt;&lt;tspan x=&#34;0&#34; y=&#34;21.6001&#34;&gt;实&lt;/tspan&gt;&lt;tspan x=&#34;18&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.5em;&#34;&gt;：&lt;/tspan&gt;&lt;tspan x=&#34;27&#34; y=&#34;21.6001&#34;&gt;客观事&lt;/tspan&gt;&lt;tspan x=&#34;81&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.019em;&#34;&gt;实&lt;/tspan&gt;&lt;tspan x=&#34;98.6582&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.481em;&#34;&gt;、&lt;/tspan&gt;&lt;tspan x=&#34;107.9995&#34; y=&#34;21.6001&#34;&gt;主观意见&lt;/tspan&gt;&lt;tspan x=&#34;0&#34; y=&#34;43.2002&#34;&gt;知&lt;/tspan&gt;&lt;tspan x=&#34;18&#34; y=&#34;43.2002&#34; style=&#34;letter-spacing: -.5em;&#34;&gt;：&lt;/tspan&gt;&lt;tspan x=&#34;27&#34; y=&#34;43.2002&#34;&gt;知识&lt;/tspan&gt;&lt;tspan x=&#34;0&#34; y=&#34;64.7998&#34;&gt;境&lt;/tspan&gt;&lt;tspan x=&#34;18&#34; y=&#34;64.7998&#34; style=&#34;letter-spacing: -.5em;&#34;&gt;：&lt;/tspan&gt;&lt;tspan x=&#34;27&#34; y=&#34;64.7998&#34;&gt;语言所处的环境&lt;/tspan&gt;&lt;/text&gt;
    &lt;text transform=&#34;translate(557.9234 309.3947)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;(1)形式&lt;/tspan&gt;&lt;tspan x=&#34;58.2114&#34; y=&#34;0&#34; style=&#34;letter-spacing: -.5em;&#34;&gt;：&lt;/tspan&gt;&lt;tspan x=&#34;67.2114&#34; y=&#34;0&#34;&gt;名&lt;/tspan&gt;&lt;tspan x=&#34;0&#34; y=&#34;21.6001&#34;&gt;(2)语义&lt;/tspan&gt;&lt;tspan x=&#34;58.2114&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.5em;&#34;&gt;：&lt;/tspan&gt;&lt;tspan x=&#34;67.2114&#34; y=&#34;21.6001&#34;&gt;名+实&lt;/tspan&gt;&lt;tspan x=&#34;0&#34; y=&#34;43.2002&#34;&gt;(3)推理&lt;/tspan&gt;&lt;tspan x=&#34;58.2114&#34; y=&#34;43.2002&#34; style=&#34;letter-spacing: -.5em;&#34;&gt;：&lt;/tspan&gt;&lt;tspan x=&#34;67.2114&#34; y=&#34;43.2002&#34;&gt;名+实+知&lt;/tspan&gt;&lt;tspan x=&#34;0&#34; y=&#34;64.7998&#34;&gt;(4)语用&lt;/tspan&gt;&lt;tspan x=&#34;58.2114&#34; y=&#34;64.7998&#34; style=&#34;letter-spacing: -.5em;&#34;&gt;：&lt;/tspan&gt;&lt;tspan x=&#34;67.2114&#34; y=&#34;64.7998&#34;&gt;名+实+知+境&lt;/tspan&gt;&lt;/text&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/div&gt;&lt;p class=&#34;svg-title&#34; style=&#34;text-align:center; margin-top: 0; margin-bottom:0&#34;&gt;图2&amp;emsp;自然语言处理涉及的研究对象&lt;/p&gt;
&lt;p&gt;  随着涉及的研究对象越来越多，自然语言处理的研究由浅入深，可以分为形式、语义、推理和语用四个层次。形式方面主要研究语言符号层面的处理，研究的是“名”与“名”之间的关系，如通过编辑距离等计算文本之间的相似度。语义方面主要研究语言符号和其背后所要表达的含义之间的关系，即“名&amp;quot;和&amp;quot;实”之间的关系，如“手机余额不足”和“电话欠费了”两个句子的表达方式完全不同,但是背后阐述的事实是相同的。语义问题也是自然语言处理领域目前主要关注的问题。推理是在语义研究的基础之上，进一步引入知识的运用，因此涉及“名”“实”和“知”之间关系，这一点正体现了自然语言的知识性。而语用则最为复杂,由于引入了语言所处的环境因素，通常表达的是“言外之意”和“弦外之音”，同时涉及了“名“”实”“知”“境”四个方面。例如，同样的一句话“你真讨厌”，从字面意义上明显是贬义，而如果是情侣之间的对话，则含义可能就不一样了。另外，语气、语调以及说话人的表情和动作也会影响其要表达的含义。&lt;/p&gt;
&lt;h2 id=&#34;自然语言处理技术发展历史&#34;&gt;自然语言处理技术发展历史
&lt;/h2&gt;&lt;p&gt;  自然语言处理自诞生之日起经历了两大研究范式的转换，即理性主义和经验主义，如&lt;a class=&#34;link&#34; href=&#34;#20241113004321CST&#34;&gt;图3&lt;/a&gt;所示。受到语料规模以及计算能力的限制，早期的自然语言处理主要采用基于理性主义的规则方法，通过专家总结的符号逻辑知识处理通用的自然语言现象。然而，由于自然语言的复杂性，基于理性主义的规则方法在面对实际应用场景中的问题时显得力不从心。&lt;/p&gt;
&lt;span id=&#34;20241113004321CST&#34;&gt;&lt;/span&gt;&lt;div class=&#34;svg-figure&#34;&gt;
            &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;svg id=&#34;_图层_1&#34; data-name=&#34;图层_1&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 863.7 216.3853&#34;&gt;
  &lt;text transform=&#34;translate(113.9812 15.8398)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;小规模专家知识&lt;/tspan&gt;&lt;tspan x=&#34;9.4141&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;（&lt;/tspan&gt;&lt;tspan x=&#34;27&#34; y=&#34;21.6001&#34;&gt;理性主&lt;/tspan&gt;&lt;tspan x=&#34;81&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;义&lt;/tspan&gt;&lt;tspan x=&#34;98.5859&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.477em;&#34;&gt;）&lt;/tspan&gt;&lt;tspan x=&#34;-55.3135&#34; y=&#34;43.2002&#34;&gt;20世纪50年代—20世纪90年代&lt;/tspan&gt;&lt;/text&gt;
  &lt;text transform=&#34;translate(326.5991 171.0249)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;大规模语料库统计模型&lt;/tspan&gt;&lt;tspan x=&#34;36.4141&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;（&lt;/tspan&gt;&lt;tspan x=&#34;54&#34; y=&#34;21.6001&#34;&gt;经验主&lt;/tspan&gt;&lt;tspan x=&#34;108&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;义&lt;/tspan&gt;&lt;tspan x=&#34;125.5859&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.477em;&#34;&gt;）&lt;/tspan&gt;&lt;tspan x=&#34;-9.7017&#34; y=&#34;43.2002&#34;&gt;20世纪50年代—21世纪初&lt;/tspan&gt;&lt;/text&gt;
  &lt;text transform=&#34;translate(510.5938 15.8401)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;大规模语料库深度学习&lt;/tspan&gt;&lt;tspan x=&#34;36.4141&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;（&lt;/tspan&gt;&lt;tspan x=&#34;54&#34; y=&#34;21.6001&#34;&gt;经验主&lt;/tspan&gt;&lt;tspan x=&#34;108&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;义&lt;/tspan&gt;&lt;tspan x=&#34;125.5859&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.477em;&#34;&gt;）&lt;/tspan&gt;&lt;tspan x=&#34;30.5283&#34; y=&#34;43.2002&#34;&gt;2010年-2017年&lt;/tspan&gt;&lt;/text&gt;
  &lt;text transform=&#34;translate(639.4346 171.0248)&#34; style=&#34;font-family: NotoSerifSC-ExtraLight, &amp;apos;Noto Serif SC&amp;apos;; font-size: 18px; font-weight: 200;&#34;&gt;&lt;tspan x=&#34;0&#34; y=&#34;0&#34;&gt;大规模预训练语言模型&lt;/tspan&gt;&lt;tspan x=&#34;36.4141&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;（&lt;/tspan&gt;&lt;tspan x=&#34;54&#34; y=&#34;21.6001&#34;&gt;经验主&lt;/tspan&gt;&lt;tspan x=&#34;108&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.023em;&#34;&gt;义&lt;/tspan&gt;&lt;tspan x=&#34;125.5859&#34; y=&#34;21.6001&#34; style=&#34;letter-spacing: -.477em;&#34;&gt;）&lt;/tspan&gt;&lt;tspan x=&#34;43.7764&#34; y=&#34;43.2002&#34;&gt;2018年至今&lt;/tspan&gt;&lt;/text&gt;
  &lt;g style=&#34;opacity: .2;&#34;&gt;
    &lt;polygon points=&#34;340.075 105.1447 74.9321 105.1447 44.2655 87.6844 309.4083 87.6844 340.075 105.1447&#34;/&gt;
    &lt;polygon points=&#34;559.1226 105.1447 349.2178 105.1447 314.9321 87.6844 524.8369 87.6844 559.1226 105.1447&#34;/&gt;
    &lt;polygon points=&#34;681.9797 105.1447 568.2655 105.1447 532.5829 87.6844 646.2972 87.6844 681.9797 105.1447&#34;/&gt;
    &lt;polygon points=&#34;804.8369 105.1447 691.1226 105.1447 653.5988 87.6844 767.3131 87.6844 804.8369 105.1447&#34;/&gt;
    &lt;polygon points=&#34;340.075 105.1447 74.9321 105.1447 44.2655 122.605 309.4083 122.605 340.075 105.1447&#34;/&gt;
    &lt;polygon points=&#34;559.1226 105.1447 349.2178 105.1447 314.9321 122.605 524.8369 122.605 559.1226 105.1447&#34;/&gt;
    &lt;polygon points=&#34;681.9797 105.1447 568.2655 105.1447 532.5829 122.605 646.2972 122.605 681.9797 105.1447&#34;/&gt;
    &lt;polygon points=&#34;804.8369 105.1447 691.1226 105.1447 653.5988 122.605 767.3131 122.605 804.8369 105.1447&#34;/&gt;
  &lt;/g&gt;
  &lt;g style=&#34;opacity: .2;&#34;&gt;
    &lt;line x1=&#34;178.3678&#34; y1=&#34;71.0495&#34; x2=&#34;178.3678&#34; y2=&#34;87.6844&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
    &lt;circle cx=&#34;178.3678&#34; cy=&#34;71.0495&#34; r=&#34;2.2857&#34;/&gt;
  &lt;/g&gt;
  &lt;g style=&#34;opacity: .2;&#34;&gt;
    &lt;line x1=&#34;424.5512&#34; y1=&#34;139.2399&#34; x2=&#34;424.5512&#34; y2=&#34;122.605&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
    &lt;circle cx=&#34;424.5512&#34; cy=&#34;139.2399&#34; r=&#34;2.2857&#34;/&gt;
  &lt;/g&gt;
  &lt;g style=&#34;opacity: .2;&#34;&gt;
    &lt;line x1=&#34;595.4254&#34; y1=&#34;71.0495&#34; x2=&#34;595.4254&#34; y2=&#34;87.6844&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
    &lt;circle cx=&#34;595.4254&#34; cy=&#34;71.0495&#34; r=&#34;2.2857&#34;/&gt;
  &lt;/g&gt;
  &lt;g style=&#34;opacity: .2;&#34;&gt;
    &lt;line x1=&#34;729.2178&#34; y1=&#34;139.2399&#34; x2=&#34;729.2178&#34; y2=&#34;122.605&#34; style=&#34;fill: none; stroke: #000; stroke-miterlimit: 10;&#34;/&gt;
    &lt;circle cx=&#34;729.2178&#34; cy=&#34;139.2399&#34; r=&#34;2.2857&#34;/&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/div&gt;&lt;p class=&#34;svg-title&#34; style=&#34;text-align:center; margin-top: 0; margin-bottom:0&#34;&gt;图3&amp;emsp;自然语言处理技术发展阶段&lt;/p&gt;
&lt;p&gt;  从20世纪90年代开始，随着计算机运算速度和存储容量的快速增加，以及 统计学习方法的愈发成熟，使得以语料库为核心的统计学习方法在自然语言处理领域得以大规模应用。由于大规模的语料库中包含了大量关于语言的知识，使得基于语料库的统计自然语言处理方法能够更加客观、准确和细致地捕获语言规律。在这一时期，词法分析、句法分析、信息抽取、机器翻译和自动问答等领域的研究均取得了一定程度的成功。&lt;br&gt;
  尽管基于统计学习的自然语言处理取得了一定程度的成功，但它也有明显的局限性，也就是需要事先利用经验性规则将原始的自然语言输入转化为机器能够处理的向量形式。这一转化过程（也称为特征提取）需要细致的人工操作和一定的专业知识，因此也被称为&lt;strong&gt;特征工程&lt;/strong&gt;。&lt;br&gt;
  2010年之后，随着&lt;strong&gt;基于深度神经网络的表示学习方法&lt;/strong&gt;（也称&lt;strong&gt;深度学习&lt;/strong&gt;）的兴起，该方法直接端到端地学习各种自然语言处理任务，不再依赖人工设计的特征。所谓&lt;strong&gt;表示学习&lt;/strong&gt;，是指机器能根据输入自动地发现可以用于识别或分类等任务的表示。具体地，深度学习模型在结构上通常包含多层的处理层。底层的处理层接收原始输入，然后对其进行抽象处理，其后的每一层都在前一层的结果上进行更深层次的抽象，最后一层的抽象结果即为输入的一个表示，用于最终的目标任务。其中的抽象处理，是由模型内部的参数进行控制的，而参数的更新值则是根据训练数据上模型的表现，使用反向传播算法学习得到的。由此可以看出，深度学习可以有效地避免统计学习方法中的人工特征提取操作，自动地发现对于目标任务有效的表示。在语音识别、计算机视觉等领域，深度学习已经取得了目前最好的效果，在自然语言处理领域，深度学习同样引发了一系列的变革。&lt;br&gt;
  除了可以自动地发现有效特征，表示学习方法的另一个好处是打通了不同任
务之间的壁垒。传统统计学习方法需要针对不同的任务设计不同的特征，这些特征往往是无法通用的。而表示学习能够将不同任务在相同的向量空间内进行表示,从而具备跨任务迁移的能力。除了可以跨任务，还可以实现跨语言甚至跨模态的迁移。综合利用多项任务、多种语言和多个模态的数据，使得人工智能向更通用的方向迈进了一步。&lt;br&gt;
  同样，得益于深度学习技术的快速发展，自然语言处理的另一个主要研究方 向——&lt;strong&gt;自然语言生成&lt;/strong&gt;也取得了长足进步。长期以来，自然语言生成的研究几乎处于停滞状态，除了使用模板生成一些简单的语句，并没有什么太有效的解决办法。随着基于深度学习的序列到序列生成框架的提出，这种逐词的文本生成方法全面提升了生成技术的灵活性和实用性，完全革新了机器翻译、文本摘要和人机对话等任务的技术范式。&lt;br&gt;
  虽然深度学习技术大幅提高了自然语言处理系统的准确率，但是基于深度学 习的算法有一个致命的缺点，就是过度依赖于大规模有标注数据。对于语音识别、图像处理等感知类任务，标注数据相对容易获得，如：在图像处理领域，人们已经为上百万幅的图像标注了相应的类别（如Image Net数据集）；用于语音识别的“语音—文本”平行语料库也有几十万小时。然而，由于自然语言处理这一认知类任务所具有的&amp;quot;主观性”特点，以及其所面对的任务和领域众多，使得标注大规模语料库的时间过长，人力成本过于高昂，因此自然语言处理的标注数据往往不够充足，很难满足深度学习模型训练的需要。&lt;br&gt;
  早期的静态词向量预训练模型，以及后来的动态词向量预训练模型，特别是 2018年以来，以BERT、GPT为代表的超大规模预训练语言模型恰好弥补了自然语言处理标注数据不足的缺点，帮助自然语言处理取得了一系列的突破，使得包括阅读理解在内的所有自然语言处理任务的性能都得到了大幅提高，在有些数据 集上达到或甚至超过了人类水平。&lt;br&gt;
  所谓&lt;strong&gt;模型预训练（Pre-train）&lt;/strong&gt;，即首先在一个原任务上预先训练一个初始模型，然后在下游任务（也称目标任务）上继续对该模型进行&lt;strong&gt;精调（Fine-tune）&lt;/strong&gt;，从而达到提高下游任务准确率的目的。在本质上，这也是&lt;strong&gt;迁移学习（Transfer Learning）&lt;/strong&gt;‌思想的一种应用。然而，由于同样需要人工标注，导致原任务标注数据的规模往往也非常有限。那么，如何获得更大规模的标注数据呢？&lt;br&gt;
  其实，文本自身的顺序性就是一种天然的标注数据，通过若干连续出现的词 语预测下一个词语（又称语言模型）就可以构成一项原任务。由于图书、网页等文本数据规模近乎无限，所以，可以非常容易地获得超大规模的预训练数据。有人将这种不需要人工标注数据的预训练学习方法称为&lt;strong&gt;无监督学习（Unsupervised Learning）&lt;/strong&gt;，其实这并不准确，因为学习的过程仍然是&lt;strong&gt;有监督的（Supervised）&lt;/strong&gt;，更准确的叫法应该是&lt;strong&gt;自监督学习（Self-supervised Learning）&lt;/strong&gt;。&lt;br&gt;
  为了能够刻画大规模数据中复杂的语言现象，还要求所使用的深度学习模型
容量足够大。基于自注意力的Transformer模型显著地提升了对于自然语言的建模能力，是近年来具有里程碑意义的进展之一。要想在可容忍的时间内，在如此大规模的数据上训练一个超大规模的Transformer模型，也离不开以GPU、TPU为代表的现代并行计算硬件。可以说，超大规模预训练语言模型完全依赖“蛮力”，在大数据、大模型和大算力的加持下，使自然语言处理取得了长足的进步。如OpenAI推出的GPT-3，是一个具有1750亿个参数的巨大规模，无须接受任何特定任务的训练，便可以通过小样本学习完成十余种文本生成任务，如问答、风格迁移、网页生成和自动编曲等。目前，预训练模型已经成为自然语言处理的新范式。&lt;/p&gt;
&lt;blockquote&gt;
    &lt;p&gt;&lt;div style=&#34;text-align: center;&#34;&gt;
人类不感谢罗辑。
&lt;/div&gt;
&lt;/p&gt;&lt;span class=&#34;cite&#34;&gt;&lt;span&gt;― &lt;/span&gt;&lt;span&gt;刘慈欣, &lt;/span&gt;&lt;cite&gt;《三体Ⅲ：死神永生》&lt;/cite&gt;&lt;/span&gt;&lt;/blockquote&gt;
</description>
        </item>
        
    </channel>
</rss>

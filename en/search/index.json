[{"content":" ","date":"2024-09-23T19:43:23+08:00","permalink":"https://www.brasswrench.com/en/posts/artificial_intelligence/computer_vision/frontier_topic_gaussian_splatting/basic_principles/","title":"Gaussian Splatting(Ⅰ): Basic Principles of the 3D Gaussian Splatting Algorithm"},{"content":" ","date":"2024-09-19T22:28:40+08:00","permalink":"https://www.brasswrench.com/en/posts/computer_science/operating_system/introduction_to_operating_system/","title":"Operating System(Ⅰ): Introduction to operating system"},{"content":" Markov Process Introduction There is a class of stochastic processes that possesses the so-called \u0026ldquo;memoryless property\u0026rdquo; (Markov property), which means to determine the future state of the process, knowing its current state is sufficient without needing to know its past states. Such processes are called Markov processes. This article will introduce the two simplest types of Markov processes: the discrete-time Markov chain (abbreviated as Markov chain) and the continuous-time Markov chain.\nBasic Concepts Definition of Markov Chain Markov Chain Definition 1: Given a stochastic process \\( \\{X_n, n=0,1,2,\\cdots\\} \\), if it takes only a finite or countably infinite set of values \\( E_0, E_1, E_2, \\cdots \\) (we label \\( E_0, E_1, E_2 \\) as \\( \\{0, 1, 2, \\cdots\\} \\) and call them the states of the process, the set \\( \\{0, 1, 2 \\cdots\\} \\) or its subset is referred to as the state space). If for \\( \\{X_n, n=0,1,2,\\cdots\\} \\) (generally considering its states are non-negative integers) and any \\( n \\geq 0 \\) and states \\( i, j, i_0, i_1 \\cdots, i_{n-1} \\), we have\n$$ \\begin{aligned} \u0026P\\{X_{n+1}=j \\;|\\; X_0=i_0, X_1=i_1, X_2=i_2, \\cdots, X_{n-1}=i_{n-1}, X_n=i\\}\\\\[5pt] =\u0026P\\{X_{n+1}=j \\;|\\; X_n=i\\} \\end{aligned} $$ then the stochastic process \\( \\{X_n, n=0,1,2,\\cdots\\} \\) is called a Markov chain.\nMarkov Property Definition 1 characterizes the property of the Markov chain, thus called the Markov property.\nTransition Probability Transition Probability Definition 2: The conditional probability \\( P\\{X_{n+1}=j\\;|\\;X_n=i\\} \\) in Definition 1 is called the one-step transition probability of the Markov chain \\( \\{X_n,n=0,1,2,\\cdots\\} \\), abbreviated as transition probability.\nIn general, the transition probability depends on states \\( i, j \\) and time \\( n \\).\nTime-Homogeneous Markov Chain Definition 3: If the transition probability of a Markov chain depends only on states \\( i, j \\) and not on \\( n \\), the Markov chain is called time-homogeneous, denoted as \\( p_{ij}=P\\{X_{n+1}=j\\;|\\;X_n=i\\} (n \\geq 0) \\); otherwise, it is called non-time-homogeneous.\nThis article only discusses time-homogeneous Markov chains and refers to them simply as Markov chains.\nFinite Chain and Infinite Chain When the states of a Markov chain are finite, it is called a finite chain; otherwise, it is called an infinite chain. In either case, we can arrange \\( p_{ij} (i,j \\in S) \\) in the form of a matrix, denoted as\n$$ \\mathbf{P}=(p_{ij})= \\begin{pmatrix} p_{00} \u0026 p_{01} \u0026 p_{02} \u0026 p_{03} \u0026 \\cdots \\\\ p_{10} \u0026 p_{11} \u0026 p_{12} \u0026 p_{13} \u0026 \\cdots \\\\ p_{20} \u0026 p_{21} \u0026 p_{22} \u0026 p_{23} \u0026 \\cdots \\\\ p_{30} \u0026 p_{31} \u0026 p_{32} \u0026 p_{33} \u0026 \\cdots \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \\\\ \\end{pmatrix} $$ then \\( \\mathbf{P} \\) is called the transition probability matrix, often referred to as the transition matrix. It is evident that \\( p_{ij} (i,j \\in S) \\) has the properties\n$$ \\begin{aligned} \u0026(1)\\; p_{ij} \\geq 0, \\;i,j\\in S \\\\[5pt] \u0026(2)\\; \\sum_{j \\in S}{p_{ij}} = 1, \\forall i \\in S \\end{aligned} $$n-Step Transition Probability and C-K Equation n-Step Transition Probability Definition 4: The conditional probability\n$$ p_{ij}^{(n)}=P\\{X_{m+n}=j\\;|\\;X_m=i\\}\\quad i,j\\in S,m \\geq 0, n \\geq 1 $$is called the n-step transition probability of the Markov chain, and the corresponding \\( \\mathbf{P}^{(n)}=(p_{ij}^{(n)}) \\) is called the n-step transition matrix.\nWhen \\( n=1 \\), \\( p_{ij}^{(1)}=p_{ij} \\), \\( \\mathbf{P}^{(1)}=\\mathbf{P} \\). Moreover, it is stipulated that\n$$ p_{ij}^{(0)}= \\begin{cases} 0, \\quad i \\neq j\\\\[5pt] 1, \\quad i=j \\end{cases} $$ Clearly, the n-step transition probability \\( p_{ij}^{(n)} \\) refers to the probability of the system transitioning from state \\( i \\) to state \\( j \\) after \\( n \\) steps, regardless of the intermediate states in the \\( n-1 \\) steps. The following theorem gives the relationship between \\( p_{ij}^{(n)} \\) and \\( p_{ij} \\).\nC-K Equation Theorem 1 (Chapman-Kolmogorov Equation, abbreviated C-K Equation): For all \\( n,m \\geq 0 \\), \\( i,j \\in S \\),\n①\n$$ p_{ij}^{(m+n)}=\\sum_{k \\in S}{p_{ik}^{(m)} p_{kj}^{(n)}} $$ ②\n$$ \\mathbf{P}^{(n)}=\\mathbf{P} \\cdot \\mathbf{P}^{(n-1)} = \\mathbf{P} \\cdot \\mathbf{P} \\cdot \\mathbf{P}^{(n-2)} = \\cdots = \\mathbf{P}^n $$ Proof (Theorem 1): By the law of total probability, we have\n$$ \\begin{aligned} p_{ij}^{(m+n)}\u0026=P\\{X_{m+n}=j\\;|\\;X_0=i\\}\\\\[5pt] \u0026=\\frac{P\\{X_{m+n}=j, X_0=i\\}}{P\\{X_0=i\\}}\\\\[10pt] \u0026=\\sum_{k \\in S}{\\frac{P\\{X_{m+n}=j,X_m=k,X_0=i\\}}{P\\{X_0=i\\}}} \\text{(law of total probability)}\\\\ \u0026=\\sum_{k \\in S}{\\frac{P\\{X_{m+n}=j,X_m=k,X_0=i\\}}{P\\{X_m=k,X_0=i\\}}\\frac{P\\{X_m=k,X_0=i\\}}{P\\{X_0=i\\}}}\\\\ \u0026=\\sum_{k \\in S}{P\\{X_{m+n}=j\\;|\\;X_m=k,X_0=i\\}}P\\{X_m=k\\;|\\;X_0=i\\}\\\\ \u0026=\\sum_{k \\in S}{p_{kj}^{(n)} p_{ik}^{(m)}}\\\\ \u0026=\\sum_{k \\in S}{p_{ik}^{(m)} p_{kj}^{(n)}} \\end{aligned} $$ From matrix multiplication, it is easy to see that ② is a matrix form of ①.\nClassification and Properties of States Reachable, Communicating, Class, Reducible Definition 5: A state \\( i \\) is said to reach state \\( j \\) (\\( i,j \\in S \\)) if there exists \\( n \\geq 0 \\) such that \\( p_{ij}^{(n)} \\geq 0 \\), denoted as \\( i \\rightarrow j \\). If there is also \\( j \\rightarrow i \\), then \\( i \\) and \\( j \\) are said to communicate, denoted as \\( i \\leftrightarrow j \\).\nTheorem 2: Communication is an equivalence relation, which satisfies:\n① Reflexivity: \\( i \\leftrightarrow i \\);\n② Symmetry: \\( i \\leftrightarrow j \\), then \\( j \\leftrightarrow i \\);\n③ Transitivity: \\( i \\leftrightarrow j \\), \\( j \\leftrightarrow k \\), then \\( i \\leftrightarrow k \\).\nProof (Theorem 2): It is obvious that ① and ② are true from the definition of communication, so we only need to prove ③. It follows from \\( i \\rightarrow j \\), \\( j \\rightarrow k \\) that there exist \\( m, n \\geq 0 \\) such that \\( p_{ij}^{(m)} \u003e 0 \\) and \\( p_{jk}^{(n)} \u003e 0 \\). Since\n$$ p_{ik}^{(m+n)} \\geq p_{ij}^{(m)} p_{jk}^{(n)} $$we have \\( p_{ik}^{(m+n)} \u003e 0 \\), which means \\( i \\rightarrow k \\). The proof for \\( k \\rightarrow i \\) is similar, completing the proof of ③, thus proving Theorem 2.\nWe classify any two communicating states into one class. According to the above theorem, states in the same class should communicate with each other, and any state cannot belong to two different classes simultaneously.\nDefinition 6: A Markov chain is called irreducible if it has only one class. Otherwise, it is called reducible.\nPeriod Definition 7: If the set \\( \\{n \\mid n \\ge 1, p_{ii}^{(n)} \\ge 0\\} \\) exists, then the greatest common divisor \\( d=d(i) \\) of this set is called the period of state \\( i \\). If \\( d \\ge 1 \\), then \\( i \\) is periodic; if \\( d=1 \\), then \\( i \\) is aperiodic. It is specially stipulated that when the above set is empty, the period of \\( i \\) is said to be infinite.\nNote: Although \\( i \\) has a period \\( d \\), it does not mean that \\( p_{ii}^{(nd)} \\) is greater than \\( 0 \\) for all \\( n \\). However, it can be proven that \\( p_{ii}^{(nd)}\u003e0 \\) when \\( n \\) is sufficiently large (see proof ).\nTheorem 3: If states \\( i \\) and \\( j \\) belong to the same class, then \\( d(i)=d(j) \\).\nProof (Theorem 3): From the definition of class, we know \\( i \\leftrightarrow j \\), which means there exist \\( m, n \\) such that \\( p_{ij}^{(m)}\u003e0 \\) and \\( p_{ji}^{(n)}\u003e0 \\). Therefore, by the C-K equation and the non-negativity of probability , we have \\( p_{ii}^{(m+n)}=\\sum_{k \\in S}{p_{ik}^{(m)} p_{ki}^{(n)}} \\geq p_{ij}^{(m)}p_{ji}^{(n)}\u003e0 \\). Also, for all \\( s \\) such that \\( p_{jj}^{(s)}\u003e0 \\), we have \\( p_{ii}^{(m+s+n)}=\\sum_{k \\in S}{p_{ik}^{(m)}p_{ki}^{(s+n)}}=\\sum_{k \\in S, l \\in S}{p_{ik}^{(m)}p_{kl}^{(s)}p_{li}^{(n)}} \\geq p_{ij}^{(m)}p_{jj}^{(s)}p_{ji}^{(n)}\u003e0 \\). According to the period definition, since \\( p_{ii}^{(m+n)}\u003e0 \\) and \\( p_{ii}^{(m+s+n)}\u003e0 \\), \\( d(i) \\) must divide both \\( n+m \\) and \\( n+m+s \\), and thus \\( d(i) \\) must also divide \\( s \\) (see the basic properties of divisibility ). Note that we have assumed \\( p_{jj}^{(s)}\u003e0 \\), so \\( d(j) \\) must also divide \\( s \\). Considering the arbitrariness of \\( s \\), we can directly set \\( s=d(j) \\), thus obtaining that \\( d(i) \\) divides \\( d(j) \\). By swapping the roles of \\( i \\) and \\( j \\) in the above proof, we can also obtain that \\( d(j) \\) divides \\( d(i) \\), hence \\( d(i)=d(j) \\) (see the basic properties of divisibility).\nRecurrence Recurrence and Transience Definition 8: For any states \\( i,j \\), let \\( f_{ij}^{(n)} \\) denote the probability that starting from \\( i \\), the process reaches \\( j \\) for the first time in \\( n \\) steps. Then,\n$$ f_{ij}^{(n)}= \\begin{cases} 0 \u0026 n=0 \\\\[5pt] P\\{X_n=j,\\; X_k \\ne j \\text{ for } k=1,2,\\cdots ,n-1 \\mid X_0=i\\} \u0026 n \\ge 1 \\end{cases} $$ Let \\( f_{ij}=\\sum_{n=1}^{\\infty}{f_{ij}^{(n)}} \\). If \\( f_{jj}=1 \\), then state \\( j \\) is called a recurrent state; if \\( f_{jj}\u003c1 \\), then \\( j \\) is called a transient state.\nThe meaning of \\( f_{ij} \\) can be derived as follows: From the definition of the set \\( A_n \\), \\( A_n=\\{X_n=j,\\; X_k \\ne j \\text{ for } k=1,2,\\cdots ,n-1 \\mid X_0=i\\} \\), we know that different \\( A_n \\) are disjoint. The event \\( \\bigcup_{n=1}^{\\infty}{A_n} \\) represents the event that there is always at least one \\( n \\) such that the process reaches \\( j \\) from \\( i \\) in \\( n \\) steps. By the additivity of probabilities of disjoint events, we get:\n$$ P(\\bigcup_{n=1}^{\\infty}{A_n})=\\sum_{n=1}^{\\infty}{P(A_n)}=\\sum_{n=1}^{\\infty}f_{ij}^{(n)}=f_{ij} $$ Therefore, \\( f_{ij} \\) represents the probability that the process can reach \\( j \\) from \\( i \\) within a finite number of steps. When \\( i \\) is a recurrent state, starting from \\( i \\), it will return to \\( i \\) with probability 1 within a finite number of steps. When \\( i \\) is a transient state, starting from \\( i \\), it will with probability \\( 1-f_{ii} \\) never return to \\( i \\) (i.e., it will pass through \\( i \\)).\nFor a recurrent state \\( i \\), define\n$$ \\mu_i=\\sum_{n=1}^{\\infty}{nf_{ii}^{(n)}} $$ which represents the expected number of steps (time) needed to return to \\( i \\) starting from \\( i \\).\nPositive Recurrence, Null Recurrence, Ergodicity, Absorption Definition 9: For a recurrent state \\( i \\), if \\( \\mu_i\u003c+\\infty \\), then \\( i \\) is called a positive recurrent state; if \\( \\mu_i=+\\infty \\), then \\( i \\) is called a null recurrent state. Specifically, if \\( i \\) is positively recurrent and aperiodic, then it is called an ergodic state. If \\( i \\) is an ergodic state and \\( f_{ii}^{(1)}=1 \\), it is called an absorbing state. In this case, it is clear that \\( \\mu_i=1 \\).\nProofs of Some Properties Recurrence Limits Determination Theorem 4: State $i$ is a recurrent state if and only if $\\displaystyle\\sum_{n=0}^{\\infty}{p_{ii}^{(n)}}=+\\infty$ . When state $i$ is transient,\n$$ \\sum_{n=0}^{\\infty}{p_{ii}^{(n)}}=\\frac{1}{1-f_{ii}} $$ Thus, we have $\\lim_{n \\rightarrow \\infty}p_{ii}^{(n)}=0$ in this case.\nTo prove Theorem 4, we need to first prove Conclusion 1 and Lemma 1.\nConclusion 1: For any states $i,j$,\n$$ f_{ij}^{(l+1)}=\\sum_{k \\ne j,\\, k \\in S}{f_{ik}^{(1)}f_{kj}^{(l)}} $$ Proof (Conclusion 1): By the definition of $f_{ij}^{(n)}$, Markov property, time-homogeneity, and the Total Probability Theorem,\n$$ \\begin{aligned} f_{ij}^{(l+1)}=\u0026P\\{X_{l+1}=j, X_m \\ne j (m=1,2,\\cdots ,l) \\;|\\; X_0=i\\}\\quad (\\text{definition of } f_{ij}^{(n)})\\\\[5pt] =\u0026\\sum_{k \\ne j,\\, k \\in S}{P\\{X_{l+1}=j, X_m \\ne j (m=2,\\cdots ,l), X_1=k \\;|\\; X_0=i\\}} \\, (\\text{Total Probability Theorem})\\\\ =\u0026\\sum_{k \\ne j,\\, k \\in S}{\\frac{P\\{X_{l+1}=j, X_m \\ne j (m=2,\\cdots ,l), X_1=k, X_0=i\\}}{P\\{X_0=i\\}}}\\\\ =\u0026\\sum_{k \\ne j,\\, k \\in S}{\\frac{P\\{X_1=k, X_0=i\\}}{P\\{X_0=i\\}}\\frac{P\\{X_{l+1}=j, X_m \\ne j (m=2,\\cdots ,l), X_1=k, X_0=i\\}}{P\\{X_1=k, X_0=i\\}}}\\\\ =\u0026\\sum_{k \\ne j,\\, k \\in S}{P\\{X_1=k\\;|\\;X_0=i\\}P\\{X_{l+1}=j, X_m \\ne j (m=2,\\cdots ,l)\\;|\\;X_1=k, X_0=i\\}}\\\\ =\u0026\\sum_{k \\ne j,\\, k \\in S}{P\\{X_1=k\\;|\\;X_0=i\\}P\\{X_{l+1}=j, X_m \\ne j (m=2,\\cdots ,l)\\;|\\;X_1=k\\}} \\, (\\text{Markov property})\\\\ =\u0026\\sum_{k \\ne j,\\, k \\in S}{P\\{X_1=k\\;|\\;X_0=i\\}P\\{X_l=j, X_m \\ne j (m=1,\\cdots ,l-1)\\;|\\;X_0=k\\}} \\, (\\text{time-homogeneity})\\\\ =\u0026\\sum_{k \\ne j,\\, k \\in S}{f_{ik}^{(1)}f_{kj}^{(l)}} \\end{aligned} $$ Lemma 1: For any states $i,j$ and $1 \\le n \u0026lt; \\infty$,\n$$ p_{ij}^{(n)}=\\sum_{l=1}^{n}{f_{ij}^{(l)}p_{jj}^{(n-l)}} $$ Proof (Lemma 1): Using mathematical induction. For $n=1$, since $p_{ij}^{(1)}=f_{ij}^{(1)}$, it is easy to prove that Equation (14) holds.\nAssuming for $n-1$, we have $p_{ij}^{(n-1)}=\\displaystyle\\sum_{l=1}^{n-1}{f_{ij}^{(l)}p_{jj}^{(n-1-l)}}$.\nWhen $n$ is taken, using the C-K equation, the inductive hypothesis, and Conclusion 1, we can derive\n$$ \\begin{aligned} p_{ij}^{(n)}\u0026=\\sum_{k \\in S}{p_{ik}^{(1)}p_{kj}^{(n-1)}} \\quad (\\text{C-K equation})\\\\ \u0026=p_{ij}^{(1)}p_{jj}^{(n-1)}+\\sum_{k \\ne j,\\, k \\in S}{p_{ik}^{(1)}p_{kj}^{(n-1)}}\\\\ \u0026=f_{ij}^{(1)}p_{jj}^{(n-1)}+\\sum_{k \\ne j,\\, k \\in S}{f_{ik}^{(1)}p_{kj}^{(n-1)}} \\, (\\text{for } n=1)\\\\ \u0026=f_{ij}^{(1)}p_{jj}^{(n-1)}+\\sum_{k \\ne j,\\, k \\in S}{f_{ik}^{(1)}\\left(\\sum_{l=1}^{n-1}{f_{kj}^{(l)}p_{jj}^{(n-1-l)}}\\right)} \\, (\\text{inductive hypothesis for } n-1)\\\\ \u0026=f_{ij}^{(1)}p_{jj}^{(n-1)}+\\sum_{l=1}^{n-1}{\\left(\\sum_{k \\ne j,\\, k \\in S}{f_{ik}^{(1)}f_{kj}^{(l)}}\\right)p_{jj}^{(n-1-l)}}\\\\ \u0026=f_{ij}^{(1)}p_{jj}^{(n-1)}+\\sum_{l=1}^{n-1}{f_{ij}^{(l+1)}p_{jj}^{(n-1-l)}} \\, (\\text{Lemma 1)}\\\\ \u0026=f_{ij}^{(1)}p_{jj}^{(n-1)}+\\sum_{l=2}^{n}{f_{ij}^{(l)}p_{jj}^{(n-l)}}\\\\ \u0026=\\sum_{l=1}^{n}{f_{ij}^{(l)}p_{jj}^{(n-l)}} \\end{aligned} $$ Now, we can use Lemma 1 to prove Theorem 4.\nProof (Theorem 4):\n$$ \\begin{aligned} \\sum_{n=0}^{\\infty}{p_{ii}^{(n)}}=\u0026\\:p_{ii}^{(0)}+\\sum_{n=1}^{\\infty}{p_{ii}^{(n)}}\\\\ =\u0026\\:1+\\sum_{n=1}^{\\infty}\\left(\\sum_{l=1}^{n}{f_{ii}^{(l)}p_{ii}^{(n-l)}}\\right) \\, (\\text{Lemma 1})\\\\ =\u0026\\:1+\\sum_{l=1}^{\\infty}\\sum_{n=l}^{\\infty}{f_{ii}^{(l)}p_{ii}^{(n-l)}}\\\\ =\u0026\\:1+\\sum_{l=1}^{\\infty}\\sum_{m=0}^{\\infty}{f_{ii}^{(l)}p_{ii}^{(m)}}\\\\ =\u0026\\:1+\\left(\\sum_{l=1}^{\\infty}{f_{ii}^{(l)}}\\right)\\left(\\sum_{n=0}^{\\infty}{p_{ii}^{(n)}}\\right)\\\\ =\u0026\\:1+f_{ii}\\left(\\sum_{n=0}^{\\infty}{p_{ii}^{(n)}}\\right) \\end{aligned} $$ With the same term on both sides, $\\displaystyle\\sum_{n=0}^{\\infty}{p_{ii}^{(n)}}$, we solve the equation to get\n$$ \\sum_{n=0}^{\\infty}{p_{ii}^{(n)}}=\\frac{1}{1-f_{ii}} $$ Therefore,\n$$ \\sum_{n=0}^{\\infty}{p_{ii}^{(n)}}\\text{ converges if and only if } f_{ii}\u003c1\\text{;} \\sum_{n=0}^{\\infty}{p_{ii}^{(n)}}\\text{ diverges iff } f_{ii}=1 $$Communicating Recurrent States are Achievable Lemma 2: If $i \\leftrightarrow j$ and $i$ is a recurrent state, then $f_{ji}=1$.\nProof (Lemma 2): By contradiction. Suppose $f_{ji}\u0026lt;1$, then starting from $j$ does not necessarily reach $i$ in a finite number of steps. But since $i \\rightarrow j$, starting from $i$ must reach $j$ in finite steps. This means that if the process goes from $i$ through $j$, it will not necessarily return to $i$ in finite steps, which contradicts the recurrence of $i$. Therefore, $f_{ij}=1$ is established.\nRecurrence is a Class Property Theorem 5: Recurrence is a class property.\nProof (Theorem 5): First, prove that if $i \\leftrightarrow j$, then both $i,j$ are either recurrent states or transient states.\nSince $i \\leftrightarrow j$, there exist $n, m$ such that $p_{ij}^{(n)}\u0026gt;0$, $p_{ji}^{(m)}\u0026gt;0$, by the C-K equation and the expansion technique used in Theorem 3 proof, we have\n$$ p_{ii}^{(n+m+l)} \\ge p_{ij}^{(n)}p_{jj}^{(l)}p_{ji}^{(m)}\\\\[5pt] p_{jj}^{(n+m+l)} \\ge p_{ji}^{(n)}p_{ii}^{(l)}p_{ij}^{(m)} $$ Summing both sides yields\n$$ \\sum_{l=0}^{\\infty}{p_{ii}^{(n+m+l)}} \\ge \\sum_{l=0}^{\\infty}{p_{ij}^{(n)}p_{jj}^{(l)}p_{ji}^{(m)}}\\\\[5pt] \\sum_{l=0}^{\\infty}{p_{jj}^{(n+m+l)}} \\ge \\sum_{l=0}^{\\infty}{p_{ji}^{(n)}p_{ii}^{(l)}p_{ij}^{(m)}} $$ Considering that $\\displaystyle\\sum_{l=0}^{n+m-1}{p_{ii}^{(l)}}$ and $\\displaystyle\\sum_{l=0}^{n+m-1}{p_{jj}^{(l)}}$ are both finite, we can write the above as:\n$$ \\sum_{l=0}^{\\infty}{p_{ii}^{(l)}} - \\sum_{l=0}^{n+m-1}{p_{ii}^{(l)}} \\ge p_{ij}^{(n)}p_{ji}^{(m)}\\sum_{l=0}^{\\infty}{p_{jj}^{(l)}}\\\\[5pt] \\sum_{l=0}^{\\infty}{p_{jj}^{(l)}} - \\sum_{l=0}^{n+m-1}{p_{jj}^{(l)}} \\ge p_{ji}^{(n)}p_{ij}^{(m)}\\sum_{l=0}^{\\infty}{p_{ii}^{(l)}} $$ Thus, we can intuitively see that if $\\displaystyle\\sum_{l=0}^{n+m-1}{p_{ii}^{(l)}}$ and $\\displaystyle\\sum_{l=0}^{n+m-1}{p_{jj}^{(l)}}$ are infinite, the other must also be infinite; if one is finite, the other is also finite. Thus, both $i$ and $j$ are either recurrent or transient states. Therefore, recurrence is a class property, such that if any member of the class satisfies it, all other members satisfy it; conversely, if any member of the class does not satisfy it, all other members do not.\nFurthermore, if $i,j$ are both recurrent states, they are either both positively recurrent or null recurrent. The proof for this is given in proof below.\nState Space Decomposition Theorem Theorem 6 (State Space Decomposition Theorem): The state space $S$ of any Markov chain can be uniquely decomposed into a finite or countable number of mutually exclusive subsets $D,C_1,C_2,\\cdots$ such that:\n(1) Each $C_n$ is a closed set consisting of recurrent states, and $D$ consists of all transient states.\n(2) States within $C_n$ are of the same type, either all positively recurrent or all null recurrent. They have the same period, and $f_{ij}=1$ for all $i,j \\in C_n$.\n(3) States within $C_n$ cannot reach any state in $D$.\nProof (Theorem 6): Let $C$ be the set of all recurrent states, then $D=S-C$ is the set of all transient states. Noting that the communication within $C$ is an equivalence relation, by equivalence relations and set partitioning, $C$ can be partitioned into $C_1 \\cup C_2 \\cup \\cdots$, where each $C_n$ is a closed set consisting of recurrent states that communicate. Thus, $S=D \\cup C_1 \\cup C_2 \\cup \\cdots$. Assumptions and (1) of the theorem are proved. By Theorem 5, states within $C_n$ are all of the same type (either positively or null recurrent), and by Theorem 3, states within $C_n$ have the same period. By Lemma 2, $f_{ij}=1$ for all $i,j \\in C_n$. (2) is proved. For (3), it can be proven by contradiction. Suppose a state $i$ in $C_n$ can reach a state $j$ in $D$. Since $i$ is recurrent, after reaching $j$, the process must return to $i$ again, and by the assumption, it might return to $j$ again. Therefore, starting from $j$, it is possible to return to $j$, which contradicts the transient nature of $j$. Thus, the assumption is false. Hence, states within $C_n$ cannot reach any state in $D$. (3) is proved. Theorem 6 is thus proved.\nIrreducible Markov Chain Transitions Theorem 7: For an irreducible Markov chain with period \\( d \\), its state space \\( S \\) can uniquely be decomposed into \\( d \\) mutually disjoint subsets, that is\n$$ S=\\bigcup_{r=0}^{d-1}{S_r},\\quad S_r \\bigcap S_s=\\varnothing,\\quad r \\ne S $$Furthermore, starting from any state in \\( S_r \\), the next transition in 1 step must fall into \\( S_{r+1} \\) (where \\( S_d=S_0 \\)).\nProof (Theorem 7): Let\u0026rsquo;s first define the subsets \\( S_r \\):\nArbitrarily choose a state \\( i \\), and for each \\( r=0,1,\\cdots,d-1 \\), define the set\n$$ S_r=\\{j\\;|\\; \\text{there exists } n \\ge 0 \\text{ such that } p_{ij}^{(nd+r)}\u003e0\\} $$Since \\( S \\) is irreducible, we know that starting from \\( i \\), every state in \\( S \\) can be visited, and \\( nd+r (r=0,1,\\cdots,d-1) \\) can cover each time state starting from \\( i \\). Hence, the union of all \\( S_r \\) must be \\( S \\), i.e., \\(\\bigcup_{r=0}^{d-1}{S_r}=S\\).\nNext, we prove that \\( S_r \\) are disjoint. Using a proof by contradiction, suppose there exist \\( S_r, S_s \\) and a state \\( j \\) such that \\( j \\in S_r \\bigcap S_s \\). According to the definition of \\( S_r \\), there exist \\( n, m \\) such that \\( p_{ij}^{(nd+r)}\u003e0 \\) and \\( p_{ij}^{(md+s)}\u003e0 \\). Since \\( i \\leftrightarrow j \\), there exists \\( h \\) such that \\( p_{ji}^{(h)}\u003e0 \\). Using the Chapman-Kolmogorov equations and the technique used in Theorem 3 proof for expansion, we get:\n$$ p_{ii}^{(nd+r+h)} \\ge p_{ij}^{(nd+r)} p_{ji}^{(h)}\u003e0\\\\[5pt] p_{ii}^{(md+s+h)} \\ge p_{ij}^{(md+s)} p_{ji}^{(h)}\u003e0 $$According to the definition of period, both \\( nd+r+h \\) and \\( md+s+h \\) must be divisible by \\( d \\). Skipping the initial \\( n \\) and \\( m \\), we get that \\( r+h \\) and \\( s+h \\) must be divisible by \\( d \\). Hence, their difference \\( (r+h)-(s+h)=r-s \\) must be divisible by \\( d \\) (see the properties of divisibility in elementary number theory). Given \\( 0 \\le r \\le d-1, 0 \\le s \\le d-1 \\), \\( 0 \\le r-s \\le d-1 \\), yielding \\( r-s \\) can only be \\( 0 \\), which means \\( r=s \\). Therefore, \\( S_r = S_s \\), contradicting the assumption, so \\( S_r \\) are disjoint.\nNext, we prove that starting from any state in \\( S_r \\), the next transition in 1 step must fall into \\( S_{r+1} \\) (where \\( S_d=S_0 \\)). This is equivalent to proving that for any \\( j \\in S_r \\), we have \\( \\sum_{k \\in S_{r+1}}{p_{jk}^{(1)}}=1 \\). In fact, according to the definition of \\( S_r \\), \\( p_{ij}^{(nd+r)}\u003e0 \\). From the definition of \\( S_r \\), we know that for \\( k \\notin S_{r+1} \\), we must have \\( p_{ik}^{(nd+r+1)}=0 \\). Therefore, using the Chapman-Kolmogorov equations and the technique used in Theorem 3 proof for expansion, we deduce:\n$$ 0=p_{ik}^{(nd+r+1)} \\ge p_{ij}^{(nd+r)} p_{jk}^{(1)}\u003e0 $$Hence, \\( p_{jk}^{(1)}=0 \\). Thus,\n$$ 1 = \\sum_{k \\in S}{p_{jk}} = \\sum_{k \\in S_{r+1}}{p_{jk}^{(1)}} + \\sum_{k \\notin S_{r+1}}{p_{jk}^{(1)}} = \\sum_{k \\in S_{r+1}}{p_{jk}^{(1)}} $$Finally, we prove the uniqueness of the decomposition. This is equivalent to proving that \\( \\{S_k\\} \\) is independent of the initial \\( i \\). That is, for any \\( i, i^{\\prime} \\), the decompositions \\( \\{S_k\\}, \\{S_k^{\\prime}\\} \\) generated by them are necessarily equal.\nFirst, we prove that for any subset \\( S_r \\) in \\( \\{S_k\\} \\), there exists a subset \\( S_s^{\\prime} \\) which exactly matches its states. First, we prove that all \\( j \\in S_r \\) can only belong to a fixed set in \\( \\{S_k^{\\prime}\\} \\). Suppose \\( i^{\\prime} \\) satisfies \\( i^{\\prime} \\in S_s \\). Because starting from any state in \\( S_k \\) it must transfer to \\( S_{k+1} \\) in 1 step, when \\( s \\le r \\), the transition from \\( i^{\\prime} \\) to \\( j \\) can only happen in the following steps:\n$$ \\begin{cases} S_s \\rightarrow S_{s+1} \\rightarrow \\cdots \\rightarrow S_r\\\\ S_s \\rightarrow S_{s+1} \\rightarrow \\cdots \\rightarrow S_r \\rightarrow S_{r+1} \\rightarrow \\cdots \\rightarrow S_{d-1} \\rightarrow S_0 \\rightarrow S_1 \\rightarrow \\cdots \\rightarrow S_r\\\\ S_s \\rightarrow S_{s+1} \\rightarrow \\cdots \\rightarrow S_r \\rightarrow \\overbrace{S_{r+1} \\rightarrow \\cdots \\rightarrow S_{d-1} \\rightarrow S_0 \\rightarrow S_1 \\rightarrow \\cdots \\rightarrow S_r}^{\\text{repeat 2 times}}\\\\ \\quad \\vdots \\end{cases} $$That is, starting from \\( i^{\\prime} \\), \\( j \\) can only be reached in \\( r-s, r-s+d, r-s+2d, \\ldots \\) steps. We notice this aligns with the definition of \\( S_{r-s}^{\\prime} \\). Hence, \\( j \\in S_{r-s}^{\\prime} \\). Conversely, for any \\( j^{\\prime} \\in S_{r-s}^{\\prime} \\), the transition from \\( i \\) to \\( i^{\\prime} \\) requires \\( s \\) steps, and from \\( i^{\\prime} \\) to \\( j^{\\prime} \\) requires \\( r-s \\) steps, making the total steps from \\( i \\) to \\( j^{\\prime} \\) is \\( r \\), thus \\( j^{\\prime} \\in S_r \\). Therefore, \\( S_r \\) and \\( S_{r-s}^{\\prime} \\) are exactly equivalent.\nSimilarly, when \\( s \u003e r \\), using the same methods, we can prove that starting from \\( i^{\\prime} \\), \\( j \\) can only be reached in \\( d-(s-r), 2d-(s-r), \\cdots \\) steps, making \\( j \\) fall into \\( S_{d-(s-r)}^{\\prime} \\). Again, it can be proved that \\( S_r \\) and \\( S_{d-(s-r)}^{\\prime} \\) are exactly equivalent. In summary, we have\n$$ \\begin{cases} S_r \\text{ and } S_{r-s}^{\\prime} \\text{ are exactly equivalent, if } r \\ge s \\\\[5pt] S_r \\text{ and } S_{d-(s-r)}^{\\prime} \\text{ are exactly equivalent, if } r \u003c s \\end{cases} $$Thus, we can easily prove that \\( \\{S_k\\} \\) and \\( \\{S_k^{\\prime}\\} \\) correspond one-to-one. Hence, the decomposition is unique.\nLimit Theorem and Invariant Distribution Limit Theorem Theorem 8 (Limit Theorem): If state \\( j \\) is a recurrent state with period \\( d \\), then\n$$ \\lim\\limits_{n \\rightarrow \\infty} p_{jj}^{(nd)} = \\frac{d}{\\mu_j} $$Proof (Theorem 8): For \\( n \\ge 0 \\), let:\n$$ r_n=\\sum_{v=n+1}^{\\infty}{f_v} $$where \\( f_v=f_{jj}^{(v)} \\). Then,\n$$ \\begin{aligned} \\sum_{n=0}^{\\infty} r_n =\u0026 \\sum_{n=0}^{\\infty} \\sum_{v=n+1}^{\\infty} f_v\\\\ =\u0026(f_1 + f_2 + f_3 + \\cdots) + (f_2 + f_3 + \\cdots) + (f_3 + \\cdots) + \\cdots\\\\[5pt] =\u00261 \\cdot f_1 + 2 \\cdot f_2 + 3 \\cdot f_3 + \\cdots\\\\ =\u0026\\sum_{n=1}^{\\infty} n f_n = \\mu_j \\end{aligned} $$From the definition of \\( r_n \\), we have \\( f_n=r_{v-1}-r_v \\). Substituting this into Lemma 1 and denoting \\( p_v = p_{jj}^{(v)} \\), we get\n$$ p_n=p_{jj}^{(n)} = \\sum_{l=1}^{n} f_{jj}^{(l)} p_{jj}^{(n-l)} = -\\sum_{v=1}^{n} (r_v - r_{v-1}) p_{n-v} $$Note that \\( j \\) is a recurrent state, so \\( r_0=1 \\). The above formula can then be written as\n$$ \\sum_{v=0}^{n} r_v p_{n-v} = \\sum_{v=0}^{n-1} r_v p_{n-1-v} $$This formula indicates a very clear conclusion: \\( \\sum_{v=0}^{n} r_v p_{n-v} \\) is independent of \\( n \\). Hence,\n$$ \\sum_{v=0}^{n} r_v p_{n-v} = r_0 p_0 = 1, \\quad n \\ge 0 $$Let \\( \\lambda = \\limsup\\limits_{n \\rightarrow \\infty} p_{nd} \\). Note that when \\( k \\) is not a multiple of \\( d \\), \\( p_k=0 \\), and \\( p_{nd} \\ge 0 \\). By the definition of the limit superior, we know:\n$$ \\limsup\\limits_{k \\rightarrow \\infty} p_k = \\limsup\\limits_{n \\rightarrow \\infty} p_{nd} = \\lambda $$Therefore, by the definition of the limit superior, there must be a subsequence whose limit equals \\( \\lambda \\). Note again that when \\( k \\) is not a multiple of \\( d \\), \\( p_k=0 \\). Hence, this subsequence can be taken from \\( \\{nd\\} \\). Therefore, there exists a subsequence \\( \\{n_m\\}, n_m \\rightarrow \\infty \\) such that \\( \\lambda = \\lim\\limits_{m \\rightarrow \\infty} p_{n_md} \\). For any \\( s \\) such that \\( f_s \u003e 0 \\), using Lemma 1 and the properties of the limit superior, we obtain\n$$ \\begin{aligned} \\lambda =\u0026 \\liminf\\limits_{m \\rightarrow \\infty} p_{n_md} = \\liminf\\limits_{m \\rightarrow \\infty} p_{jj}^{(n_md)} = \\liminf\\limits_{m \\rightarrow \\infty} \\sum_{v=1}^{n_md} f_{jj}^{(v)} p_{jj}^{(n_md-v)}\\\\ =\u0026 \\liminf\\limits_{m \\rightarrow \\infty} \\sum_{v=1}^{n_md} f_v p_{n_md-v} = \\liminf\\limits_{m \\rightarrow \\infty} \\left( f_s p_{n_md-s} + \\sum_{v=1, v \\ne s}^{n_md} f_v p_{n_md-v} \\right)\\\\ \\le \u0026\\liminf\\limits_{m \\rightarrow \\infty} f_s p_{n_md-s} + \\liminf\\limits_{m \\rightarrow \\infty} \\left( \\sum_{v=1, v \\ne s}^{n_md} f_v p_{n_md-v} \\right) \\quad \\text{(using limit inferior properties)} \\end{aligned} $$Using the non-negativity of probabilities and the properties of the limit superior, we infer:\n$$ \\begin{aligned} \\liminf\\limits_{m \\rightarrow \\infty} \\sum_{v=1, v \\ne s}^{n_md} f_v p_{n_md-v} \\le \u0026\\liminf\\limits_{m \\rightarrow \\infty} \\sum_{v=1, v \\ne s}^{\\infty} f_v p_{n_md-v} \\quad \\text{(non-negativity of probabilities)}\\\\ \\le \u0026\\sum_{v=1, v \\ne s}^{\\infty} \\liminf\\limits_{m \\rightarrow \\infty} f_v p_{n_md-v} \\quad \\text{(using limit inferior properties)}\\\\ \\le \u0026\\sum_{v=1, v \\ne s}^{\\infty} \\liminf\\limits_{m \\rightarrow \\infty} f_v \\limsup\\limits_{m \\rightarrow \\infty} p_{n_md-v} \\quad \\text{(property of products)}\\\\ =\u0026\\sum_{v=1, v \\ne s}^{\\infty} f_v \\limsup\\limits_{m \\rightarrow \\infty} p_{n_md-v}\\\\ \\le \u0026\\sum_{v=1, v \\ne s}^{\\infty} f_v \\limsup\\limits_{m \\rightarrow \\infty} p_m \\quad \\text{(definition of limit superior)}\\\\ =\u0026\\left( \\sum_{v=1, v \\ne s}^{\\infty} f_v \\right) \\limsup\\limits_{m \\rightarrow \\infty} p_m \\end{aligned} $$Note that state \\( j \\) is recurrent, so \\( \\sum_{v=1}^{\\infty} f_v=1 \\), meaning \\( \\sum_{v=1, v \\ne s}^{\\infty} f_v = 1 - f_s \\). As previously stated, \\( \\lambda = \\limsup\\limits_{m \\rightarrow \\infty} p_m \\). Substituting this back into \\( \\lambda \\) yields:\n$$ \\lambda \\le f_s \\liminf\\limits_{m \\rightarrow \\infty} p_{n_md-s} + (1 - f_s) \\lambda $$Thus,\n$$ \\liminf\\limits_{m \\rightarrow \\infty} p_{n_md-s} \\ge \\lambda\n","date":"2024-09-14T21:05:01+08:00","permalink":"https://www.brasswrench.com/en/posts/mathematics/stochastic_processes/markov_process/","title":"Random Process (Ⅳ): Markov Process"}]